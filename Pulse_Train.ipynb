{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "8t16MutY8ARr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load CSV files into pandas DataFrames\n",
        "def load_data(file_paths):\n",
        "    dataframes = [pd.read_csv(file) for file in file_paths]\n",
        "    return dataframes"
      ],
      "metadata": {
        "id": "8w14RXI48Cvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess data\n",
        "def preprocess_data(dataframes):\n",
        "    # Assuming the last column is the target variable\n",
        "    X = pd.concat([df.iloc[:, :-1] for df in dataframes], axis=0)\n",
        "    y = pd.concat([df.iloc[:, -1] for df in dataframes], axis=0)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "mmB8ptTf8E-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def create_model(input_shape):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "vrC7ThbH9BVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train models\n",
        "def train_models(file_paths):\n",
        "    # Load CSV files\n",
        "    dataframes = load_data(list(file_paths.values()))\n",
        "\n",
        "    # Preprocess data\n",
        "    X_dict = {}\n",
        "    y_dict = {}\n",
        "    for label, path in file_paths.items():\n",
        "        X_dict[label], y_dict[label] = preprocess_data([dataframes[i] for i, p in enumerate(file_paths.values()) if p == path])\n",
        "\n",
        "    # Split data into train and test sets for each type\n",
        "    X_train_dict = {}\n",
        "    X_test_dict = {}\n",
        "    y_train_dict = {}\n",
        "    y_test_dict = {}\n",
        "    for label in file_paths.keys():\n",
        "        X_train_dict[label], X_test_dict[label], y_train_dict[label], y_test_dict[label] = train_test_split(X_dict[label], y_dict[label], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train and evaluate a model for each type\n",
        "    types = [\"Cycler\", \"Jitter\", \"Stable\", \"Stagger\"]\n",
        "    models = {}\n",
        "    accuracies = {}\n",
        "    for label in types:\n",
        "        print(f\"Training Model for Type: {label}\")\n",
        "        input_shape = (X_train_dict[label].shape[1],)\n",
        "        model = create_model(input_shape)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        model.fit(X_train_dict[label], y_train_dict[label], epochs=10, batch_size=32, validation_data=(X_test_dict[label], y_test_dict[label]))\n",
        "        models[label] = model\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(X_test_dict[label], y_test_dict[label])\n",
        "        accuracies[label] = test_acc\n",
        "        print(f\"Accuracy for Type {label}: {test_acc}\")\n",
        "\n",
        "    print(\"Overall Accuracy:\")\n",
        "    print(f\"Mean Accuracy: {np.mean(list(accuracies.values()))}\")\n",
        "    print(f\"Accuracy for Each Type: {accuracies}\")\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "w9koTHg0fqVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test a single file with all four models\n",
        "def test_single_file(file_path, models):\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(file_path)\n",
        "    X_test, y_test = preprocess_data([test_data])\n",
        "\n",
        "    # Evaluate each model on the test data\n",
        "    results = {}\n",
        "    for label, model in models.items():\n",
        "        print(f\"Evaluating Model for Type: {label}\")\n",
        "        test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "        results[label] = test_acc\n",
        "        print(f\"Accuracy for Type {label}: {test_acc}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# File paths for training\n",
        "train_file_paths = {\n",
        "    \"Cycler\": \"cycler_data.csv\",\n",
        "    \"Jitter\": \"jitter_data.csv\",\n",
        "    \"Stable\": \"stable_data.csv\",\n",
        "    \"Stagger\": \"stagger_data.csv\"\n",
        "}\n",
        "\n",
        "# File path for testing\n",
        "test_file_path = \"test_data.csv\"\n",
        "\n",
        "# Train models\n",
        "models = train_models(train_file_paths)\n",
        "\n",
        "# Test single file with all four models\n",
        "results = test_single_file(test_file_path, models)\n",
        "print(\"Accuracy for Each Type:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "oV3C51tGfwbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}